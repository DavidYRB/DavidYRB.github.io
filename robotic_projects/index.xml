<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Project | Ruibo Yan</title>
    <link>https://DavidYRB.github.io/robotic_projects/</link>
      <atom:link href="https://DavidYRB.github.io/robotic_projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Project</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>@ 2021 by Ruibo Yan</copyright><lastBuildDate>Thu, 27 Sep 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://DavidYRB.github.io/media/icon_hu0e21efafdcd6577cacaebca3a6409662_13407_512x512_fill_lanczos_center_3.png</url>
      <title>Project</title>
      <link>https://DavidYRB.github.io/robotic_projects/</link>
    </image>
    
    <item>
      <title>Origami Soft Snake</title>
      <link>https://DavidYRB.github.io/robotic_projects/origamisnake/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/origamisnake/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Snake robots offer a useful and unique mobility platform for search-and-rescue applications. However, existing prototypes made of rigid links and joints are hampered by a lack of flexibility that limits their utility in highly cluttered, maze-like environments, and their heavy weight limits their energy-efficiency and performance in three-dimensional (3-D) tasks. As an application based on the research of origami modular manipulator, we created an OriSnake to perform two types (undulation and sidewinding) locomotion that are common seen from real snakes. The OriSnake is assembled by four cylindrical, origami continuum modules driven by internal cables and electric motors, as well as a local feedback control system on each module. Thus, we can distribute actuation, sensing, and control for highly scalable soft robotic continuum origami systems.&lt;/p&gt;
&lt;h3 id=&#34;main-contribution&#34;&gt;Main Contribution&lt;/h3&gt;
&lt;p&gt;The contribution of this work mainly related to the WPI OriSnake including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating a Origami Snake robot that is capable of both lateral undulation and side-winding.&lt;/li&gt;
&lt;li&gt;Analyzing the behavior of the OriSnake under different
locomotion parameters and search for optimal set of parameters
for maximum linear speed for both locomotion gaits.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Manipulation Hardware</title>
      <link>https://DavidYRB.github.io/robotic_projects/ar/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/ar/</guid>
      <description>&lt;p&gt;As a co-op, I worked on projects related to end-effectors which will be used to solve the picking problems. I created a ROS Python package to control newly-designed end-effectors to perform picking behavior.&lt;/p&gt;
&lt;p&gt;Afterwards, I set up a two-roboarm test station from installing station frames, configuring network hub connections, to  calibrating camera sensors. To display the station in ROS, I created an entire set of URDF files so the station can be accurately described and displayed in Rviz.&lt;/p&gt;
&lt;p&gt;I later expanded the ROS stack used internally to facilitate the test process of end-effectors. By creating a package that enables people to select picking points with desired angle and position in the pot through a real time 3D image on the screen, with just a click, the system will drive to the selected location and pick the object.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modular Soft Manipulator</title>
      <link>https://DavidYRB.github.io/robotic_projects/origamimodule/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/origamimodule/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Continuum manipulators, robot limbs inspired by trunks, snakes, and tentacles, represent a promising field in robotic manipulation research. They are well known for their compliance, as they can conform to the shape of objects they interact with. Furthermore, they also benefit from dexterity and reduced weight compared to traditional rigid manipulators. This research aims to create and evaluate an origami-inspired cable driven continuum manipulator module that offers low-cost, low volume deployment, light weight, and inherently safe human interaction and collaboration.&lt;/p&gt;
&lt;h3 id=&#34;main-contribution&#34;&gt;Main Contribution&lt;/h3&gt;
&lt;p&gt;In summary, the contribution of this work is the development and analysis of a new approach to continuum
manipulation that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uses an origami-inspired mechanism as its body, allowing for significant extension/contraction and bending
motions;&lt;/li&gt;
&lt;li&gt;Has a high torsional strength, allowing it to resist
undesired twisting deformations in 3-D space;&lt;/li&gt;
&lt;li&gt;Is composed of self-contained modules, whereby the
addition of modules requires minimal design changes.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Udacity Self-driving Car Nanodegree</title>
      <link>https://DavidYRB.github.io/robotic_projects/self_driving/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/self_driving/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Developed a PID controller and a MPC controller  to drive a vehicle in a simulator track (Control)&lt;/li&gt;
&lt;li&gt;Programmed a particle filter to localize a vehicle with sensor information with RSME less than 0.4 (Localization)&lt;/li&gt;
&lt;li&gt;Created an Extended Kalman Filter fusing Radar and Lidar data to estimate pedestrianâ€™s position with RSME less than 0.1 (Sensor Fusion)&lt;/li&gt;
&lt;li&gt;Generated a dataset of vehicles (featuring by spatial binning color, color histogram, and histogram of gradient (HOG)) and used LinearSVC to train a car classifier acquired 98.3% accuracy (Machine Learning)&lt;/li&gt;
&lt;li&gt;Detected lane lines and marked driving area with polynomial-fit boundary in video with sobel threshold and color threshold binary images from each frame (Computer Vision)&lt;/li&gt;
&lt;li&gt;Trained a traffic sign classifier (LeNet) with preprocessed 43-class (50,000+ images) on AWS with validation accuracy with 94.3% and test accuracy 94% (Deep Learning)&lt;/li&gt;
&lt;li&gt;Controlled a vehicle to drive within a track by a model (LeNet) which is trained with collected dataset of human drive (Deep Learning)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Warehouse Prototype Develpment</title>
      <link>https://DavidYRB.github.io/robotic_projects/staples/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/staples/</guid>
      <description>&lt;p&gt;The project was aiming to build a warehouse robotics stack for Staples&#39; fulfillment center. As a main participant, I built a 10m * 5m test environment with precisely allocated QR codes. By consuming the image, IR, encoder data from the prototype robot, I created a path planning algorithm and a controller that enables the warehouse robot to move from any start point to end point stably while carring a 500lb pod.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Smart Cart</title>
      <link>https://DavidYRB.github.io/robotic_projects/staples_smartcart/</link>
      <pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/staples_smartcart/</guid>
      <description>&lt;p&gt;By creating a test platform with various sensors, I led a team of five built a prototype with SLAM and self-navigation functionality. It laid fundation for further development of different modes of a shopping cart in Staples&#39; stores, for example cart-follow-person mode&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
