<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mobile | Ruibo Yan</title>
    <link>https://DavidYRB.github.io/tag/mobile/</link>
      <atom:link href="https://DavidYRB.github.io/tag/mobile/index.xml" rel="self" type="application/rss+xml" />
    <description>mobile</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>@ 2021 by Ruibo Yan</copyright><lastBuildDate>Sun, 27 Aug 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://DavidYRB.github.io/media/icon_hu0e21efafdcd6577cacaebca3a6409662_13407_512x512_fill_lanczos_center_3.png</url>
      <title>mobile</title>
      <link>https://DavidYRB.github.io/tag/mobile/</link>
    </image>
    
    <item>
      <title>Udacity Self-driving Car Nanodegree</title>
      <link>https://DavidYRB.github.io/robotic_projects/self_driving/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/self_driving/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Developed a PID controller and a MPC controller  to drive a vehicle in a simulator track (Control)&lt;/li&gt;
&lt;li&gt;Programmed a particle filter to localize a vehicle with sensor information with RSME less than 0.4 (Localization)&lt;/li&gt;
&lt;li&gt;Created an Extended Kalman Filter fusing Radar and Lidar data to estimate pedestrianâ€™s position with RSME less than 0.1 (Sensor Fusion)&lt;/li&gt;
&lt;li&gt;Generated a dataset of vehicles (featuring by spatial binning color, color histogram, and histogram of gradient (HOG)) and used LinearSVC to train a car classifier acquired 98.3% accuracy (Machine Learning)&lt;/li&gt;
&lt;li&gt;Detected lane lines and marked driving area with polynomial-fit boundary in video with sobel threshold and color threshold binary images from each frame (Computer Vision)&lt;/li&gt;
&lt;li&gt;Trained a traffic sign classifier (LeNet) with preprocessed 43-class (50,000+ images) on AWS with validation accuracy with 94.3% and test accuracy 94% (Deep Learning)&lt;/li&gt;
&lt;li&gt;Controlled a vehicle to drive within a track by a model (LeNet) which is trained with collected dataset of human drive (Deep Learning)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Warehouse Prototype Develpment</title>
      <link>https://DavidYRB.github.io/robotic_projects/staples/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/staples/</guid>
      <description>&lt;p&gt;The project was aiming to build a warehouse robotics stack for Staples&#39; fulfillment center. As a main participant, I built a 10m * 5m test environment with precisely allocated QR codes. By consuming the image, IR, encoder data from the prototype robot, I created a path planning algorithm and a controller that enables the warehouse robot to move from any start point to end point stably while carring a 500lb pod.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Smart Cart</title>
      <link>https://DavidYRB.github.io/robotic_projects/staples_smartcart/</link>
      <pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://DavidYRB.github.io/robotic_projects/staples_smartcart/</guid>
      <description>&lt;p&gt;By creating a test platform with various sensors, I led a team of five built a prototype with SLAM and self-navigation functionality. It laid fundation for further development of different modes of a shopping cart in Staples&#39; stores, for example cart-follow-person mode&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
